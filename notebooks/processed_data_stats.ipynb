{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code to create statistics on the processed cohorts in the MEDS format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "sys.path.append(os.path.join(parent_dir, \"src\"))\n",
    "\n",
    "from datasets import load_from_disk, Dataset\n",
    "from femr.hf_utils import aggregate_over_dataset\n",
    "from ehr_stats.codes import create_code_occurence_plot, create_code_stats_table, plot_codes_per_patient, extract_code_stats, combine_code_stats, get_summary_statistics\n",
    "import numpy as np\n",
    "from texttable import Texttable\n",
    "import latextable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_MIMIC_MEDS_DATA_DIR = \"/home/niclas/Dokumente/thesis-daten/mimic_meds_2.2/\"\n",
    "RAW_MIMIC_OMOP_MEDS_DATA_DIR = \"/home/niclas/Dokumente/thesis-daten/mimic-omop-meds\"\n",
    "DATA_RESULTS_DIR = \"/home/niclas/Dokumente/thesis-daten/demo_results/\"\n",
    "OUTPUT_DIR = os.path.join(DATA_RESULTS_DIR, \"calculated_output\")\n",
    "CLEAN_DATA_PATH = os.path.join(DATA_RESULTS_DIR, \"clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_code_stats(dataset):\n",
    "    code_stats = aggregate_over_dataset(dataset, extract_code_stats, combine_code_stats, 50, 2)\n",
    "    return code_stats\n",
    "\n",
    "clean_dataset = load_from_disk(CLEAN_DATA_PATH)\n",
    "clean_code_stats = calculate_code_stats(clean_dataset)\n",
    "\n",
    "create_code_stats_table(clean_code_stats, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from texttable import Texttable\n",
    "import os\n",
    "import latextable\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Function to calculate min, mean, max, and IQR (displayed as Q1-Q3)\n",
    "def calculate_statistics(data):\n",
    "    \"\"\"Calculates minimum, mean, maximum, and interquartile range (IQR) of the given data.\"\"\"\n",
    "    q1 = np.percentile(data, 25)\n",
    "    q3 = np.percentile(data, 75)\n",
    "    mean = np.mean(data)\n",
    "    return {\n",
    "        \"min\": np.min(data),\n",
    "        \"mean\": mean,\n",
    "        \"max\": np.max(data),\n",
    "        \"median\": np.median(data),\n",
    "        \"q1\": q1,\n",
    "        \"q3\": q3,\n",
    "        \"total\": np.sum(data)\n",
    "    }\n",
    "\n",
    "# Function to format timedelta in \"xyz days (x.y years)\"\n",
    "def format_timedelta(timedelta):\n",
    "    \"\"\"Formats a timedelta object to a string of days and years.\"\"\"\n",
    "    days = timedelta.days\n",
    "    hours = timedelta.seconds / 3600\n",
    "    days += hours / 24\n",
    "    years = days / 365\n",
    "    return f\"{days:,.0f} days ({years:,.1f} years)\"\n",
    "\n",
    "\n",
    "def create_dataset_stats(dataset):\n",
    "    summary_ds = dataset.map(get_summary_statistics,\n",
    "    batched=True,\n",
    "    batch_size=25,\n",
    "    remove_columns=dataset.column_names,\n",
    "    num_proc=8\n",
    ")\n",
    "    \n",
    "    # Initialize statistics dictionary and combined data storage\n",
    "    stats = {}\n",
    "    all_data = {\n",
    "        \"events_per_patient\": [],\n",
    "        \"visits_per_patient\": [],\n",
    "        \"timeline_length_per_patient\": []\n",
    "    }\n",
    "\n",
    "    # Check if the input is a DatasetDict or Dataset and compute statistics accordingly\n",
    "    if isinstance(summary_ds, DatasetDict):\n",
    "        # Process each split in the DatasetDict\n",
    "        for split in summary_ds:\n",
    "            stats[split] = {}\n",
    "            for attr in summary_ds[split].column_names:\n",
    "                data = summary_ds[split][attr]\n",
    "                stats[split][attr] = calculate_statistics(data)\n",
    "                all_data[attr].extend(data)\n",
    "\n",
    "        # Calculate statistics for all combined splits\n",
    "        stats[\"all_splits\"] = {}\n",
    "        for attr in all_data:\n",
    "            stats[\"all_splits\"][attr] = calculate_statistics(all_data[attr])\n",
    "\n",
    "        # Define splits for table generation\n",
    "        splits = ['train', 'val', 'test', 'all_splits']\n",
    "\n",
    "    else:\n",
    "        # Process a single Dataset\n",
    "        for attr in summary_ds.column_names:\n",
    "            data = summary_ds[attr]\n",
    "            stats[attr] = calculate_statistics(data)\n",
    "        \n",
    "        # Define splits for table generation when single Dataset is used\n",
    "        splits = ['all']\n",
    "\n",
    "    # Generate the summary table with statistics\n",
    "    rows = [[\"Attribute\"] + [split.capitalize() for split in splits]]\n",
    "\n",
    "    # Define the list of attributes to calculate statistics for\n",
    "    attributes = [\"events_per_patient\", \"visits_per_patient\", \"timeline_length_per_patient\"]\n",
    "\n",
    "    for attr in attributes:\n",
    "        # Add row for the attribute name\n",
    "        rows.append([f\"Number of {attr.split('_')[0].capitalize()}\"] + [\"\"] * len(splits))\n",
    "        \n",
    "        # Add rows for each metric (min, mean with IQR, max)\n",
    "        for metric in [\"min\", \"mean\", \"median\", \"max\", \"total\"]:\n",
    "            if attr == \"timeline_length_per_patient\":\n",
    "                row = [metric.capitalize()]\n",
    "                for split in splits:\n",
    "                    if metric == \"mean\":\n",
    "                        if split == 'all':\n",
    "                            mean_val = format_timedelta(stats[attr][metric])\n",
    "                            iqr_val = f\"[{format_timedelta(stats[attr]['q1'])} - {format_timedelta(stats[attr]['q3'])}]\"\n",
    "                        else:\n",
    "                            mean_val = format_timedelta(stats[split][attr][metric])\n",
    "                            iqr_val = f\"[{format_timedelta(stats[split][attr]['q1'])} - {format_timedelta(stats[split][attr]['q3'])}]\"\n",
    "                        row.append(f\"{mean_val} {iqr_val}\")\n",
    "                    else:\n",
    "                        if split == 'all':\n",
    "                            row.append(format_timedelta(stats[attr][metric]))\n",
    "                        else:\n",
    "                            row.append(format_timedelta(stats[split][attr][metric]))\n",
    "            else:\n",
    "                row = [metric.capitalize()]\n",
    "                for split in splits:\n",
    "                    if metric == \"total\" or metric == \"median\":\n",
    "                        row.append(f\"{stats[attr][metric]:,.0f}\")\n",
    "                    elif metric == \"mean\":\n",
    "                        if split == 'all':\n",
    "                            mean_val = f\"{stats[attr][metric]:,.2f}\"\n",
    "                            iqr_val = f\"[{stats[attr]['q1']:,.2f} - {stats[attr]['q3']:,.2f}]\"\n",
    "                        else:\n",
    "                            mean_val = f\"{stats[split][attr][metric]:,.2f}\"\n",
    "                            iqr_val = f\"[{stats[split][attr]['q1']:,.2f} - {stats[split][attr]['q3']:,.2f}]\"\n",
    "                        row.append(f\"{mean_val} {iqr_val}\")\n",
    "                    else:\n",
    "                        if split == 'all':\n",
    "                            row.append(f\"{stats[attr][metric]:,.2f}\")\n",
    "                        else:\n",
    "                            row.append(f\"{stats[split][attr][metric]:,.2f}\")\n",
    "            rows.append(row)\n",
    "\n",
    "    # Create and display the table using Texttable\n",
    "    table = Texttable()\n",
    "    table.set_cols_align([\"l\"] + [\"c\"] * len(splits))\n",
    "    table.set_deco(Texttable.HEADER | Texttable.VLINES)\n",
    "    table.add_rows(rows)\n",
    "    print(table.draw())\n",
    "\n",
    "    # Save the table to a LaTeX file\n",
    "    output_path = os.path.join(OUTPUT_DIR, \"summary_stats.txt\")\n",
    "    with open(output_path, 'w') as f:\n",
    "        latex_txt = latextable.draw_latex(table, caption=\"Number of inputs across data splits\")\n",
    "        f.write(latex_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_mimic_meds_dataset = Dataset.from_parquet(os.path.join(RAW_MIMIC_MEDS_DATA_DIR, \"data/*\"))\n",
    "create_dataset_stats(raw_mimic_meds_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_mimic_omop_meds_dataset = Dataset.from_parquet(os.path.join(RAW_MIMIC_OMOP_MEDS_DATA_DIR, \"data/*\"))\n",
    "create_dataset_stats(raw_mimic_omop_meds_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guo_reproduction_ds_before = load_from_disk(\"/home/niclas/Dokumente/cluster_data/biased_cohort/cohort\")\n",
    "create_dataset_stats(guo_reproduction_ds_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guo_reproduction_ds_after = load_from_disk(\"/home/niclas/Dokumente/cluster_data/biased_cohort/preprocessed\")\n",
    "create_dataset_stats(guo_reproduction_ds_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_filtering = guo_reproduction_ds_before.map(get_summary_statistics,\n",
    "    batched=True,\n",
    "    batch_size=25,\n",
    "    remove_columns=guo_reproduction_ds_before.column_names,\n",
    "    num_proc=8\n",
    ")\n",
    "\n",
    "after_filtering = guo_reproduction_ds_after.map(get_summary_statistics,\n",
    "    batched=True,\n",
    "    batch_size=25,\n",
    "    remove_columns=guo_reproduction_ds_after.column_names,\n",
    "    num_proc=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the figure and an axis for the histograms\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "before_filtering_data = np.array(before_filtering['events_per_patient'])\n",
    "after_filtering_data = np.array(after_filtering['events_per_patient'])\n",
    "\n",
    "# Define logarithmic bins\n",
    "bins = np.logspace(np.log10(before_filtering_data.min()), \n",
    "                   np.log10(before_filtering_data.max()), \n",
    "                   100)  # You can adjust the number of bins if necessary\n",
    "\n",
    "# Plot histograms for 'events_per_patient' before and after filtering with logarithmic bins\n",
    "#ax1.hist(before_filtering['events_per_patient'], bins=bins, alpha=0.4, label='Before Filtering (Count)', color='blue')\n",
    "#ax1.hist(after_filtering['events_per_patient'], bins=bins, alpha=0.4, label='After Filtering (Count)', color='orange')\n",
    "\n",
    "# Set log scale for the x-axis\n",
    "ax1.set_xscale('log')\n",
    "\n",
    "# Set labels for the histogram plot\n",
    "ax1.set_xlabel('Measurements per Patient (Log Scale)')\n",
    "ax1.set_ylabel('Density')\n",
    "\n",
    "# Create a secondary y-axis for the KDE plots\n",
    "\n",
    "# Plot KDE for 'events_per_patient' before filtering\n",
    "sns.kdeplot(before_filtering['events_per_patient'], ax=ax1, label=\"Before Code Translation\", color=\"blue\", linewidth=2, alpha=0.5, fill=True)\n",
    "\n",
    "# Plot KDE for 'events_per_patient' after filtering\n",
    "sns.kdeplot(after_filtering['events_per_patient'], ax=ax1, label=\"After Code Translation\", color=\"orange\", linewidth=2, alpha=0.5, fill=True)\n",
    "\n",
    "# Set the y-axis label for the KDE plot\n",
    "\n",
    "# Combine legends from both axes into one\n",
    "handles1, labels1 = ax1.get_legend_handles_labels()\n",
    "ax1.legend(handles1, labels1, loc='upper right')\n",
    "\n",
    "# Tight layout ensures that all plot elements fit within the figure area\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"kde_code_translation.png\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_mapping = load_from_disk(\"/home/niclas/Dokumente/cluster_data/adjusted_mapping_reduced_cohort/preprocessed\")\n",
    "create_dataset_stats(extended_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_mapping = load_from_disk(\"/home/niclas/Dokumente/cluster_data/correct_reduced_cohort/preprocessed\")\n",
    "create_dataset_stats(standard_mapping)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
