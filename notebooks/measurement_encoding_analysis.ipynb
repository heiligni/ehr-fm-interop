{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measurement Encoding Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "sys.path.append(os.path.join(parent_dir, \"src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.clmbr_t_base import get_tokenizer\n",
    "from femr.models.tokenizer import FEMRTokenizer\n",
    "from datasets import Dataset\n",
    "from femr.ontology import Ontology\n",
    "import polars as pl\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import json\n",
    "from file_paths import MAPPING_DIR, ATHENA_PATH, MIMIC_MEDS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontology = Ontology(ATHENA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Tokenizer\n",
    "We create some stats and plots to better understand what information the tokenizer can transfer into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_text_keys = set()\n",
    "text_counts = defaultdict(int)\n",
    "\n",
    "for code, text in tokenizer.string_lookup.keys():\n",
    "    simplified_text_keys.add(code)\n",
    "    text_counts[text] += 1\n",
    "\n",
    "print(len(simplified_text_keys))\n",
    "print(len(tokenizer.string_lookup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate number of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_text_tokens = len(tokenizer.string_lookup)\n",
    "num_code_tokens = len(tokenizer.code_lookup)\n",
    "num_numeric_tokens = sum(len(value) for value in tokenizer.numeric_lookup.values())\n",
    "\n",
    "print(f\"Text Tokens: {num_text_tokens}\")\n",
    "print(f\"Code Tokens: {num_code_tokens}\")\n",
    "print(f\"Numeric Tokens: {num_numeric_tokens}\")\n",
    "print(f\"Total tokens: {num_text_tokens + num_code_tokens + num_numeric_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find out why tokens are missing we dig deeper into the vocab dictionary and find out that there is a huge amount of missing tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = set()\n",
    "\n",
    "for item in tokenizer.dictionary[\"vocab\"]:\n",
    "    types.add(item[\"type\"])\n",
    "types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_stats = {\n",
    "    \"code\": 0,\n",
    "    \"text\": 0,\n",
    "    \"numeric\": 0,\n",
    "    \"unused\": 0\n",
    "}\n",
    "\n",
    "for item in tokenizer.dictionary[\"vocab\"]:\n",
    "    token_stats[item[\"type\"]] += 1\n",
    "\n",
    "token_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tokens = sum(token_stats.values())\n",
    "\n",
    "# Data with first letters capitalized\n",
    "token_stats_capitalized = {key.capitalize(): value for key, value in token_stats.items()}\n",
    "percentages_capitalized = {key.capitalize(): (value / total_tokens) * 100 for key, value in token_stats.items()}\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "bars = ax.bar(token_stats_capitalized.keys(), token_stats_capitalized.values(), color=['skyblue', 'lightgreen', 'lightcoral', 'lightsalmon'])\n",
    "\n",
    "ax.set_xlabel('Measurement Encoding Type', fontsize=12)\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adding percentages on top of the bars\n",
    "for bar, percent in zip(bars, percentages_capitalized.values()):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, height, f'{percent:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Displaying the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(MAPPING_DIR, \"tokenizer_type_dist.png\"))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert defaultdict to DataFrame\n",
    "text_count_df = pd.DataFrame(list(text_counts.items()), columns=['Text', 'Count'])\n",
    "\n",
    "# Sort the DataFrame by 'Count' in descending order\n",
    "df_sorted = text_count_df.sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_sorted)\n",
    "\n",
    "# Save DataFrame as LaTeX table\n",
    "latex_table = tabulate(df_sorted[0:10], headers='keys', tablefmt='latex', showindex=False)\n",
    "\n",
    "# Save the LaTeX table to a .tex file\n",
    "with open(os.path.join(MAPPING_DIR, 'top_10_text_counts_table.tex'), 'w') as file:\n",
    "    file.write(latex_table)\n",
    "\n",
    "print(\"LaTeX table has been saved to 'text_counts_table.tex'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer_match(measurement):\n",
    "    if measurement.get(\"numeric_value\") is not None:\n",
    "        for start, end, i in tokenizer.numeric_lookup.get(measurement[\"code\"], []):\n",
    "            if start <= measurement[\"numeric_value\"] < end:\n",
    "                return \"numeric\", i\n",
    "        else:\n",
    "            return None, None\n",
    "    elif measurement.get(\"text_value\") is not None:\n",
    "        value = tokenizer.string_lookup.get((measurement[\"code\"], measurement[\"text_value\"]))\n",
    "        if value is not None:\n",
    "            return \"text\", value\n",
    "        else:\n",
    "            return None, None\n",
    "    else:\n",
    "        value = tokenizer.code_lookup.get(measurement[\"code\"])\n",
    "        if value is not None:\n",
    "            return \"code\", value\n",
    "        else:\n",
    "            return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/niclas/Dokumente/thesis-daten/mapping-metadata/metadata.json\", \"r\") as file:\n",
    "    data_metadata = json.load(file)\n",
    "code_metadata = data_metadata[\"code_metadata\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_stats(stats1: dict, stats2: dict) -> dict:\n",
    "    result = defaultdict(int)\n",
    "    \n",
    "\n",
    "    for key, value in stats1.items():\n",
    "        result[key] += value\n",
    "\n",
    "    for key, value in stats2.items():\n",
    "        result[key] += value\n",
    "\n",
    "    return dict(result)\n",
    "\n",
    "def add_sets(set1, set2):\n",
    "    result = set1.copy()\n",
    "    result.update(set2)\n",
    "    return result\n",
    "\n",
    "def combine_extended_stats(stats1: dict, stats2: dict) -> dict:\n",
    "    return {\n",
    "        \"parent_tokens\": add_sets(stats1[\"parent_tokens\"], stats2[\"parent_tokens\"]),\n",
    "        \"direct_tokens\": add_sets(stats1[\"direct_tokens\"], stats2[\"direct_tokens\"]),\n",
    "        \"mapping_tokens\": add_sets(stats1[\"mapping_tokens\"], stats2[\"mapping_tokens\"]),\n",
    "        \"mapping_parent_tokens\": add_sets(stats1[\"mapping_parent_tokens\"], stats2[\"mapping_parent_tokens\"]),\n",
    "        \"total_counts\": combine_stats(stats1[\"total_counts\"], stats2[\"total_counts\"]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_code_stats(batch):\n",
    "    direct_ids = set()\n",
    "    parent_ids = set()\n",
    "    extended_ids = set()\n",
    "    extended_parent_ids = set()\n",
    "    stats = defaultdict(int)\n",
    "    for events in batch[\"events\"]:\n",
    "        for event in events:\n",
    "            for measurement in event[\"measurements\"]:\n",
    "                code = measurement[\"code\"]\n",
    "                stats[\"total\"] += 1\n",
    "                direct_match, id = get_tokenizer_match(measurement)\n",
    "                if direct_match is not None:\n",
    "                    direct_ids.add(id)\n",
    "                    stats[\"direct_\" + direct_match] += 1\n",
    "                    continue\n",
    "                parents = ontology.get_all_parents(code)\n",
    "                found_parent_match = False\n",
    "                for parent in parents:\n",
    "                    if parent == code:\n",
    "                        continue\n",
    "                    measurement[\"code\"] = parent\n",
    "                    parent_match, id = get_tokenizer_match(measurement)\n",
    "                    if parent_match is not None:\n",
    "                        parent_ids.add(id)\n",
    "                        found_parent_match = True\n",
    "                        stats[\"parent_\" + parent_match] += 1\n",
    "                        break\n",
    "                if not found_parent_match:\n",
    "                    mapping = code_metadata.get(code, None)\n",
    "                    if mapping is not None:\n",
    "                        found_mapping_match = False\n",
    "                        for parent in mapping.get(\"parent_codes\", []):\n",
    "                            measurement[\"code\"] = parent\n",
    "                            parent_match, id = get_tokenizer_match(measurement)\n",
    "                            if parent_match is not None:\n",
    "                                found_mapping_match = True\n",
    "                                extended_ids.add(id)\n",
    "                                stats[\"mapping_\" + parent_match] += 1\n",
    "                                break\n",
    "                        if not found_mapping_match:\n",
    "                            found_mapping_parent_match = False\n",
    "                            for mapping_parent in mapping.get(\"parent_codes\", []):\n",
    "                                parents = ontology.get_all_parents(mapping_parent)\n",
    "                                for parent in parents:\n",
    "                                    if parent == code:\n",
    "                                        continue\n",
    "                                    measurement[\"code\"] = parent\n",
    "                                    parent_match, id = get_tokenizer_match(measurement)\n",
    "                                    if parent_match is not None:\n",
    "                                        extended_parent_ids.add(id)\n",
    "                                        found_mapping_parent_match = True\n",
    "                                        stats[\"mapping_parent_\" + parent_match] += 1\n",
    "                                        break\n",
    "                            if not found_mapping_parent_match:\n",
    "                                stats[\"no_match\"] += 1\n",
    "    return {\n",
    "        \"parent_tokens\": parent_ids,\n",
    "        \"direct_tokens\": direct_ids,\n",
    "        \"mapping_tokens\": extended_ids,\n",
    "        \"mapping_parent_tokens\": extended_parent_ids,\n",
    "        \"total_counts\": dict(stats)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from femr.hf_utils import aggregate_over_dataset\n",
    "\n",
    "dataset = Dataset.from_parquet(\"/home/niclas/Dokumente/thesis-daten/mimic_meds_2.2/data/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_stats = aggregate_over_dataset(dataset, extract_code_stats, combine_extended_stats, 25, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_tokens = set(tokenizer.code_lookup.values())\n",
    "numeric_tokens = set()\n",
    "for value in tokenizer.numeric_lookup.values():\n",
    "    for _,_, id in value:\n",
    "        numeric_tokens.add(id)\n",
    "text_tokens = set(tokenizer.string_lookup.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoding_type_distributions(tokens):\n",
    "    code_count = 0\n",
    "    numeric_count = 0\n",
    "    text_count = 0\n",
    "    for token in tokens:\n",
    "        if token in code_tokens:\n",
    "            code_count += 1\n",
    "        elif token in numeric_tokens:\n",
    "            numeric_count += 1\n",
    "        elif token in text_tokens:\n",
    "            text_count += 1\n",
    "    return text_count, numeric_count, code_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categories and counts\n",
    "categories = ['Text', 'Numeric', 'Code']\n",
    "possible_matches = [2961, 11183, 25667]\n",
    "direct_matches = list(get_encoding_type_distributions(code_stats[\"direct_tokens\"]))\n",
    "parent_matches = list(get_encoding_type_distributions(code_stats[\"parent_tokens\"]))\n",
    "mapping_matches = list(get_encoding_type_distributions(code_stats[\"mapping_tokens\"]))\n",
    "mapping_parent_matches = list(get_encoding_type_distributions(code_stats[\"mapping_parent_tokens\"]))\n",
    "\n",
    "# Calculate the sum of all matches except possible matches\n",
    "sum_matches = [d + p + m + mp for d, p, m, mp in zip(direct_matches, parent_matches, mapping_matches, mapping_parent_matches)]\n",
    "\n",
    "x = np.arange(len(categories))  # Label locations\n",
    "width = 0.15  # Width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Modern color palette\n",
    "colors = {\n",
    "    'sum': '#FFAA66',  # Updated modern color for sum (light orange)\n",
    "    'possible': '#A0A0A0',  # Updated modern color for possible matches (gray)\n",
    "    'direct': '#1f77b4',  # Blue tone for direct matches\n",
    "    'parent': '#4A90E2',  # Lighter blue tone for parent matches\n",
    "    'mapping': '#2ca02c',  # Green tone for mapping matches\n",
    "    'mapping_parent': '#8DCC85'  # Lighter green tone for mapping parent matches\n",
    "}\n",
    "\n",
    "# Bars for sums of all matches (except possible matches), slightly overlapping with possible matches\n",
    "bars_sums = ax.bar(x, sum_matches, width, label='Total Matches', color=colors['sum'])\n",
    "\n",
    "# Bars for possible matches\n",
    "bars_possible = ax.bar(x, possible_matches, width, label='Possible Matches', color=colors['possible'], alpha=0.3)\n",
    "\n",
    "# Display sum values next to the sum bar\n",
    "for i, bar in enumerate(bars_sums):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2.0, height, f'{sum_matches[i]}', ha='center', va='bottom', color='black', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Bars for direct matches\n",
    "ax.bar(x + width, direct_matches, width, label='Direct Matches', color=colors['direct'])\n",
    "\n",
    "# Bars for parent matches\n",
    "ax.bar(x + 2 * width, parent_matches, width, label='Parent Matches', color=colors['parent'])\n",
    "\n",
    "# Bars for mapping matches\n",
    "ax.bar(x + 3 * width, mapping_matches, width, label='Mapping Matches', color=colors['mapping'])\n",
    "\n",
    "# Bars for mapping parent matches\n",
    "ax.bar(x + 4 * width, mapping_parent_matches, width, label='Mapping Parent Matches', color=colors['mapping_parent'])\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_xlabel('Encoding Type')\n",
    "ax.set_ylabel('Encoding Type Count')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(categories)\n",
    "ax.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.savefig(os.path.join(MAPPING_DIR, \"matches.pdf\"))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility.data import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"/home/niclas/Dokumente/cluster_data/correct_reduced_cohort/cohort\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.univeral_tokenizer import UniversalTokenizer\n",
    "\n",
    "clmbr_t_base_tokenizer = get_tokenizer(None)\n",
    "clmbr_t_mimic_tokenizer = FEMRTokenizer.from_pretrained(\"/home/niclas/Dokumente/cluster_data/pretraining_mimic/fm\")\n",
    "clmbr_t_ime_tokenizer = UniversalTokenizer.from_pretrained(\"/home/niclas/Dokumente/cluster_data/pretraining_lab/fm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer_match(measurement, tokenizer):\n",
    "    ids, _ = tokenizer.get_feature_codes(None, measurement)\n",
    "    return len(ids) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"/home/niclas/Dokumente/cluster_data/adjusted_mapping_reduced_cohort/ontology.pkl\", \"rb\") as f:\n",
    "    ontology = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_tokenizer(type):\n",
    "    if type == \"base\":\n",
    "        return clmbr_t_base_tokenizer\n",
    "    elif type == \"mimic\":\n",
    "        return clmbr_t_mimic_tokenizer\n",
    "    elif type == \"ime\":\n",
    "        return clmbr_t_ime_tokenizer\n",
    "\n",
    "def extract_mec_stats(batch):\n",
    "    stats = {\n",
    "        \"base\": {\n",
    "            \"total\": 0,\n",
    "            \"direct\": 0,\n",
    "            \"parent\": 0,\n",
    "        },\n",
    "        \"mimic\": {\n",
    "            \"total\": 0,\n",
    "            \"direct\": 0,\n",
    "            \"parent\": 0,\n",
    "        },\n",
    "        \"ime\": {\n",
    "            \"total\": 0,\n",
    "            \"direct\": 0,\n",
    "            \"parent\": 0,\n",
    "        }\n",
    "    }\n",
    "    for events in batch[\"events\"]:\n",
    "        for event in events:\n",
    "            for measurement in event[\"measurements\"]:\n",
    "                for type in [\"base\", \"mimic\", \"ime\"]:\n",
    "                    tokenizer = get_correct_tokenizer(type)\n",
    "                    stats[type][\"total\"] += 1\n",
    "                    if get_tokenizer_match(measurement, tokenizer):\n",
    "                        stats[type][\"direct\"] += 1\n",
    "                        continue\n",
    "                    parents = ontology.get_all_parents(measurement[\"code\"])\n",
    "                    for parent in parents:\n",
    "                        if parent == measurement[\"code\"]:\n",
    "                            continue\n",
    "                        measurement[\"code\"] = parent\n",
    "                        if get_tokenizer_match(measurement, tokenizer):\n",
    "                            stats[type][\"parent\"] += 1\n",
    "                            break\n",
    "    return stats\n",
    "\n",
    "def combine_mec_stats(stats1: dict, stats2: dict) -> dict:\n",
    "    return {\n",
    "        type: combine_stats(stats1[type], stats2[type]) for type in [\"base\", \"mimic\", \"ime\"]\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_stats = aggregate_over_dataset(dataset, extract_mec_stats, combine_mec_stats, 50, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for type in [\"base\", \"mimic\", \"ime\"]:\n",
    "    mec = encoding_stats[type][\"direct\"] / encoding_stats[type][\"total\"]\n",
    "    mec_o = (encoding_stats[type][\"direct\"] + 0.5 * encoding_stats[type][\"parent\"]) / encoding_stats[type][\"total\"]\n",
    "    print(f\"Type: {type}\")\n",
    "    print(f\"MEC: {mec}\")\n",
    "    print(f\"MEC_O: {mec_o}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
