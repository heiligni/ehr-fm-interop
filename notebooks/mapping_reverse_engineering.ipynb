{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OHDSI Mapping Reverse Engineering\n",
    "\n",
    "### Setup\n",
    "Import necessary files, set Python path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "sys.path.append(os.path.join(parent_dir, \"src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from femr.ontology import Ontology\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from file_paths import MAPPING_DIR, ATHENA_PATH, MIMIC_DIR\n",
    "from models.clmbr_t_base import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(None)\n",
    "ontology = Ontology(ATHENA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept = pl.scan_csv(\n",
    "    os.path.join(ATHENA_PATH, \"CONCEPT.csv\"), separator=\"\\t\", infer_schema_length=0\n",
    ")\n",
    "code_col = pl.col(\"vocabulary_id\") + \"/\" + pl.col(\"concept_code\")\n",
    "description_col = pl.col(\"concept_name\").alias(\"ontology_concept_name\")\n",
    "concept_id_col = pl.col(\"concept_id\").cast(pl.Int64).alias(\"target_concept_id\")\n",
    "processed_omop_concepts = concept.select(\n",
    "    code_col, concept_id_col, description_col\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all tokenizer codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_codes = set(tokenizer.code_lookup.keys())\n",
    "tokenizer_codes |= set(tokenizer.numeric_lookup.keys())\n",
    "tokenizer_codes |= {code for (code, _) in tokenizer.string_lookup.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_relevant_mappings(mapping_df):\n",
    "    relevant_mappings = []\n",
    "\n",
    "    for code in mapping_df[\"vocabulary_id\"].to_list():\n",
    "        if code in tokenizer_codes:\n",
    "            relevant_mappings.append(code)\n",
    "        else:\n",
    "            if any(\n",
    "                child_code in tokenizer_codes\n",
    "                for child_code in ontology.get_all_children(code)\n",
    "            ):\n",
    "                relevant_mappings.append(code)\n",
    "    return relevant_mappings\n",
    "\n",
    "\n",
    "def filter_dataframe_with_relevant_mappings(mapping_df):\n",
    "    relevant_mappings = contains_relevant_mappings(mapping_df)\n",
    "    return mapping_df.filter(pl.col(\"vocabulary_id\").is_in(relevant_mappings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"concept_name\": pl.Utf8,\n",
    "    \"source_concept_id\": pl.Int64,\n",
    "    \"target_concept_id\": pl.Int64,\n",
    "    \"source_vocabulary_id\": pl.Utf8,\n",
    "    \"source_domain_id\": pl.Utf8,\n",
    "    \"source_concept_class_id\": pl.Utf8,\n",
    "    \"concept_code\": pl.Utf8,\n",
    "}\n",
    "\n",
    "\n",
    "def check_mapping_file(filename: str):\n",
    "    name_without_ending = filename.split(\".\")[0]\n",
    "    result_dir = os.path.join(MAPPING_DIR, \"mapping_results\")\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "    summary_logs = f\"Mapping file: {filename}\\n\\n\"\n",
    "    detailed_logs = f\"\"\n",
    "\n",
    "    file_path = os.path.join(parent_dir, os.path.join(MAPPING_DIR, filename))\n",
    "\n",
    "    mapping_df = pl.read_csv(file_path, schema_overrides=schema)\n",
    "\n",
    "    processed_mappings = mapping_df.join(\n",
    "        processed_omop_concepts, on=\"target_concept_id\", how=\"left\", coalesce=True\n",
    "    )\n",
    "\n",
    "    filtered_df = filter_dataframe_with_relevant_mappings(processed_mappings)\n",
    "\n",
    "    relevant_mappings = filtered_df.shape[0]\n",
    "\n",
    "    brute_force_mappings = []\n",
    "    manual_mappings = []\n",
    "    all_relevant_mappings = []\n",
    "\n",
    "    if relevant_mappings < 10:\n",
    "        summary_logs += f\"Found {relevant_mappings} relevant mappings in tokenizer. Adding to brute force mapping list\\n\"\n",
    "        for row in filtered_df.iter_rows(named=True):\n",
    "            brute_force_mappings.append(\n",
    "                [\n",
    "                    name_without_ending,\n",
    "                    row[\"concept_code\"],\n",
    "                    row[\"vocabulary_id\"],\n",
    "                    row[\"target_concept_id\"],\n",
    "                    row[\"ontology_concept_name\"],\n",
    "                ]\n",
    "            )\n",
    "    else:\n",
    "        summary_logs += f\"Found {relevant_mappings} relevant mappings in tokenizer. Please try to manually map this file!!!\\n\"\n",
    "\n",
    "        for row in filtered_df.iter_rows(named=True):\n",
    "            manual_mappings.append(\n",
    "                [\n",
    "                    name_without_ending,\n",
    "                    row[\"concept_code\"],\n",
    "                    row[\"vocabulary_id\"],\n",
    "                    row[\"target_concept_id\"],\n",
    "                    row[\"ontology_concept_name\"],\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    for row in filtered_df.iter_rows(named=True):\n",
    "        all_relevant_mappings.append(\n",
    "            [\n",
    "                name_without_ending,\n",
    "                row[\"concept_code\"],\n",
    "                row[\"vocabulary_id\"],\n",
    "                row[\"target_concept_id\"],\n",
    "                row[\"ontology_concept_name\"],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # Define headers with specific column widths of 60 characters\n",
    "    detailed_logs += \"Found codes in tokenizer:\\n\"\n",
    "    detailed_logs += \"{:<60} {:<60} {:<60}\\n\".format(\"Code\", \"Domain\", \"OMOP Code\")\n",
    "\n",
    "    # Iterate over rows and format each row to align columns\n",
    "    for concept_code, source_domain, omop_code in filtered_df[\n",
    "        \"concept_code\", \"source_domain_id\", \"target_concept_id\"\n",
    "    ].iter_rows():\n",
    "        detailed_logs += \"{:<60} {:<60} {:<60}\\n\".format(\n",
    "            concept_code, source_domain, omop_code\n",
    "        )\n",
    "\n",
    "    with open(os.path.join(result_dir, name_without_ending + \".log\"), \"w\") as log_file:\n",
    "        log_file.write(summary_logs)\n",
    "        log_file.write(\"\\n\\n\\n\")\n",
    "        log_file.write(detailed_logs)\n",
    "\n",
    "    return (all_relevant_mappings, brute_force_mappings, manual_mappings)\n",
    "\n",
    "\n",
    "mapping_files = [f for f in os.listdir(MAPPING_DIR) if f.endswith(\".csv\")]\n",
    "\n",
    "\n",
    "brute_force_mapping_data = []\n",
    "manual_mapping_data = []\n",
    "all_relevant_mappings_data = []\n",
    "\n",
    "for filename in mapping_files:\n",
    "    (added_relevant_mappings, added_brute_force_mappings, added_manual_mappings) = (\n",
    "        check_mapping_file(filename)\n",
    "    )\n",
    "    brute_force_mapping_data.extend(added_brute_force_mappings)\n",
    "    manual_mapping_data.extend(added_manual_mappings)\n",
    "    all_relevant_mappings_data.extend(added_relevant_mappings)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    brute_force_mapping_data[1:], columns=brute_force_mapping_data[0]\n",
    ")  # data[1:] is the data, data[0] is the header\n",
    "df.to_csv(\n",
    "    os.path.join(MAPPING_DIR, \"mapping_results\", \"brute_force_mapping.csv\"), index=False\n",
    ")\n",
    "\n",
    "header = [\n",
    "    \"mapping_source_table\",\n",
    "    \"source_code\",\n",
    "    \"vocabulary_id\",\n",
    "    \"target_concept_id\",\n",
    "    \"ontology_concept_name\",\n",
    "]\n",
    "\n",
    "# Convert the final mappings list into a Polars DataFrame\n",
    "pl.DataFrame(brute_force_mapping_data, schema=header).write_csv(\n",
    "    os.path.join(MAPPING_DIR, \"mapping_results\", \"brute_force_mapping.csv\")\n",
    ")\n",
    "\n",
    "pl.DataFrame(manual_mapping_data, schema=header).write_csv(\n",
    "    os.path.join(MAPPING_DIR, \"mapping_results\", \"manual_mapping.csv\")\n",
    ")\n",
    "\n",
    "pl.DataFrame(all_relevant_mappings_data, schema=header).write_csv(\n",
    "    os.path.join(MAPPING_DIR, \"mapping_results\", \"all_relevant_mappings.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brute Force Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the Mapping File\n",
    "mapping_file = os.path.join(MAPPING_DIR, \"mapping_results\", \"brute_force_mapping.csv\")\n",
    "mapping_df = pl.read_csv(mapping_file)\n",
    "mapping_df = mapping_df.filter(\n",
    "    ~pl.col(\"mapping_source_table\").is_in([\"gcpt_vis_admission\", \"gcpt_per_ethnicity\"])\n",
    ")\n",
    "\n",
    "# Step 2: Build a Set of Source Codes\n",
    "source_codes = set(mapping_df[\"source_code\"].to_list())\n",
    "\n",
    "\n",
    "# Step 3: Function to check for source codes in a file\n",
    "def check_file_for_codes(filepath, source_codes):\n",
    "    matched_codes = []\n",
    "    print(filepath)\n",
    "    reader = pl.read_csv_batched(filepath, batch_size=25_000, infer_schema_length=0)\n",
    "    batches = reader.next_batches(20)\n",
    "    while batches:\n",
    "        df_current_batches = pl.concat(batches)\n",
    "        for col in df_current_batches.columns:\n",
    "            df_current_batches = df_current_batches.with_columns(\n",
    "                pl.col(col).cast(pl.Utf8)\n",
    "            )\n",
    "            matches = df_current_batches[col].is_in(source_codes)\n",
    "            if matches.any():\n",
    "                matched_values = df_current_batches.filter(matches)[col].unique()\n",
    "                for code in matched_values:\n",
    "                    triple = (os.path.basename(filepath), col, code)\n",
    "                    if not triple in matched_codes:\n",
    "                        matched_codes.append(triple)\n",
    "        batches = reader.next_batches(20)\n",
    "    return matched_codes\n",
    "\n",
    "\n",
    "# Step 4: Iterate Over Files in 'hosp' and 'icu' Directories\n",
    "base_dir = MIMIC_DIR\n",
    "matched_codes_summary = []\n",
    "\n",
    "for subdir in [\"hosp\", \"icu\"]:\n",
    "    dir_path = os.path.join(base_dir, subdir)\n",
    "    for root, dirs, files in os.walk(dir_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                filepath = os.path.join(root, file)\n",
    "                matches = check_file_for_codes(filepath, source_codes)\n",
    "                if matches:\n",
    "                    matched_codes_summary.extend(matches)\n",
    "\n",
    "# Step 5: Save or Display the Results\n",
    "result_df = pl.DataFrame(\n",
    "    matched_codes_summary, schema=[\"filename\", \"column\", \"matched_code\"]\n",
    ")\n",
    "result_df.write_csv(\n",
    "    os.path.join(MAPPING_DIR, \"mapping_results\", \"matched_codes_summary.csv\")\n",
    ")\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No code exists that can be mapped exactly to one code. Therefore, we ignore these mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.group_by(\"matched_code\").len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Mapping\n",
    "gcpt_drug_ndc.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data types for the necessary columns\n",
    "prescriptions_dtypes = {\"drug\": pl.Utf8, \"prod_strength\": pl.Utf8, \"ndc\": pl.Utf8}\n",
    "\n",
    "# Load only the relevant columns from the CSV file with the specified data types and row limit\n",
    "df_prescriptions = pl.read_csv(\n",
    "    os.path.join(MIMIC_DIR, \"hosp\", \"prescriptions.csv\"),\n",
    "    dtypes=prescriptions_dtypes,\n",
    "    columns=[\"drug\", \"prod_strength\", \"ndc\"],\n",
    ")\n",
    "\n",
    "# List of specific drug types\n",
    "specific_drug_types = [\n",
    "    \"Bag\",\n",
    "    \"Vial\",\n",
    "    \"Syringe\",\n",
    "    \"Syringe.\",\n",
    "    \"Syringe (Neonatal)\",\n",
    "    \"Syringe (Chemo)\",\n",
    "    \"Soln\",\n",
    "    \"Soln.\",\n",
    "    \"Sodium Chloride 0.9%  Flush\",\n",
    "]\n",
    "\n",
    "\n",
    "# Function to create gcpt_source_code\n",
    "def create_gcpt_source_code(drug, prod_strength):\n",
    "    # Determine the base drug name\n",
    "    if drug in specific_drug_types:\n",
    "        drug_name = \"\"\n",
    "    else:\n",
    "        drug_name = drug if drug is not None else \"\"\n",
    "\n",
    "    # Determine the product strength\n",
    "    prod_strength = prod_strength if prod_strength is not None else \"\"\n",
    "\n",
    "    # Concatenate and trim\n",
    "    combined = f\"{drug_name} {prod_strength}\".strip()\n",
    "\n",
    "    # If the combined string is empty, set it explicitly to an empty string\n",
    "    return combined if combined else \"\"\n",
    "\n",
    "\n",
    "# Apply the function to create the gcpt_source_code column\n",
    "df_prescriptions = df_prescriptions.with_columns(\n",
    "    [\n",
    "        pl.struct([\"drug\", \"prod_strength\"])\n",
    "        .apply(lambda row: create_gcpt_source_code(row[\"drug\"], row[\"prod_strength\"]))\n",
    "        .alias(\"concept_code\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Select the relevant fields\n",
    "df_prescriptions_filtered = df_prescriptions.select([\"drug\", \"ndc\", \"concept_code\"])\n",
    "\n",
    "# Display the result\n",
    "df_prescriptions_filtered = df_prescriptions_filtered.unique()\n",
    "\n",
    "relevant_mapping_cols = [\n",
    "    \"concept_name\",\n",
    "    \"source_concept_id\",\n",
    "    \"source_vocabulary_id\",\n",
    "    \"source_domain_id\",\n",
    "    \"source_concept_class_id\",\n",
    "    \"concept_code\",\n",
    "    \"target_concept_id\",\n",
    "    \"vocabulary_id\",\n",
    "    \"ontology_concept_name\",\n",
    "]\n",
    "drug_mapping_df = pl.read_csv(os.path.join(MAPPING_DIR, \"gcpt_drug_ndc.csv\"))\n",
    "drug_mapping_df = drug_mapping_df.join(\n",
    "    processed_omop_concepts, on=\"target_concept_id\", how=\"left\", coalesce=True\n",
    ").select(relevant_mapping_cols)\n",
    "\n",
    "joined_drug_mapping = drug_mapping_df.join(\n",
    "    df_prescriptions_filtered, on=\"concept_code\", how=\"left\"\n",
    ")\n",
    "\n",
    "# Filter out rows where both 'ndc' and 'drug' are null\n",
    "joined_drug_mapping = joined_drug_mapping.filter(\n",
    "    ~(pl.col(\"ndc\").is_null() & pl.col(\"drug\").is_null())\n",
    ")\n",
    "\n",
    "# Step 1: Apply the filter to joined_drug_mapping before counting\n",
    "filtered_ndc_df = joined_drug_mapping.filter(\n",
    "    (pl.col(\"ndc\") != \"0\") & (pl.col(\"ndc\").is_not_null())\n",
    ")\n",
    "\n",
    "# Step 2: Count the mappings for each NDC code, treating \"0\" as null\n",
    "ndc_counts = filtered_ndc_df.groupby(\"ndc\").count().rename({\"count\": \"count\"})\n",
    "\n",
    "# Step 3: Filter for NDCs with exactly one mapping\n",
    "ndc_filtered = filtered_ndc_df.join(\n",
    "    ndc_counts.filter(pl.col(\"count\") == 1), on=\"ndc\", how=\"inner\"\n",
    ")\n",
    "\n",
    "# Step 4: Apply the filter to joined_drug_mapping for drug counts\n",
    "filtered_drug_df = joined_drug_mapping.filter(\n",
    "    (pl.col(\"ndc\") == \"0\") | (pl.col(\"ndc\").is_null())\n",
    ")\n",
    "\n",
    "# Step 5: Count the mappings for each drug where NDC is \"0\" or null\n",
    "drug_counts = filtered_drug_df.groupby(\"drug\").count().rename({\"count\": \"count\"})\n",
    "\n",
    "# Step 6: Filter for drugs with exactly one mapping where NDC is \"0\" or null\n",
    "drug_filtered = filtered_drug_df.join(\n",
    "    drug_counts.filter(pl.col(\"count\") == 1), on=\"drug\", how=\"inner\"\n",
    ")\n",
    "\n",
    "# Step 7: Combine both filters (column names now match)\n",
    "combined_filtered = pl.concat([ndc_filtered, drug_filtered])\n",
    "\n",
    "# Define the table names\n",
    "table_name = \"prescriptions\"\n",
    "mapping_source_table = \"gcpt_drug_ndc\"\n",
    "\n",
    "# Iterate over the DataFrame and construct the final list\n",
    "final_mappings = []\n",
    "for row in combined_filtered.iter_rows(named=True):\n",
    "    if row[\"ndc\"] != \"0\" and row[\"ndc\"] is not None:\n",
    "        etl_prefix = \"NDC/\"\n",
    "        source_code = row[\"ndc\"]\n",
    "    else:\n",
    "        etl_prefix = \"MIMIC_IV_Drug/\"\n",
    "        source_code = row[\"drug\"]\n",
    "\n",
    "    final_mappings.append(\n",
    "        [\n",
    "            mapping_source_table,\n",
    "            table_name,\n",
    "            etl_prefix,\n",
    "            source_code,\n",
    "            row[\"vocabulary_id\"],\n",
    "            row[\"target_concept_id\"],\n",
    "            row[\"ontology_concept_name\"],\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gcpt_meas_chartevents_main_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_def_df = pl.read_csv(\n",
    "    os.path.join(MIMIC_DIR, \"icu\", \"d_items.csv\"), columns=[\"itemid\"]\n",
    ").rename({\"itemid\": \"concept_code\"})\n",
    "chartevents_mapping_df = pl.read_csv(\n",
    "    os.path.join(MAPPING_DIR, \"gcpt_meas_chartevents_main_mod.csv\")\n",
    ")\n",
    "chartevents_mapping_df = chartevents_mapping_df.join(\n",
    "    processed_omop_concepts, on=\"target_concept_id\", how=\"left\"\n",
    ")\n",
    "joined_chartevents_df = items_def_df.join(\n",
    "    chartevents_mapping_df, on=\"concept_code\", how=\"inner\"\n",
    ").select(relevant_mapping_cols)\n",
    "\n",
    "for row in joined_chartevents_df.iter_rows(named=True):\n",
    "    final_mappings.append(\n",
    "        [\n",
    "            \"gcpt_meas_chartevents_main_mod\",\n",
    "            \"d_items\",\n",
    "            \"MIMIC_IV_ITEM/\",\n",
    "            row[\"concept_code\"],\n",
    "            row[\"vocabulary_id\"],\n",
    "            row[\"target_concept_id\"],\n",
    "            row[\"ontology_concept_name\"],\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gcpt_meas_lab_loinc_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_def_df = (\n",
    "    pl.scan_csv(os.path.join(MIMIC_DIR, \"hosp\", \"d_labitems.csv\"))\n",
    "    .rename({\"itemid\": \"concept_code\"})\n",
    "    .collect()\n",
    ")\n",
    "lab_loinc_mapping_df = pl.scan_csv(\n",
    "    os.path.join(MAPPING_DIR, \"gcpt_meas_lab_loinc_mod.csv\")\n",
    ").collect()\n",
    "lab_loinc_mapping_df = lab_loinc_mapping_df.join(\n",
    "    processed_omop_concepts, on=\"target_concept_id\", how=\"left\"\n",
    ")\n",
    "joined_lab_df = lab_def_df.join(\n",
    "    lab_loinc_mapping_df, on=\"concept_code\", how=\"inner\"\n",
    ").select(relevant_mapping_cols)\n",
    "\n",
    "for row in joined_lab_df.iter_rows(named=True):\n",
    "    final_mappings.append(\n",
    "        [\n",
    "            \"gcpt_meas_lab_loinc_mod\",\n",
    "            \"d_labitems\",\n",
    "            \"MIMIC_IV_LABITEM/\",\n",
    "            row[\"concept_code\"],\n",
    "            row[\"vocabulary_id\"],\n",
    "            row[\"target_concept_id\"],\n",
    "            row[\"ontology_concept_name\"],\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gcpt_micro_microtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the microbiology_events_df with specified columns\n",
    "microbiology_events_df = pl.read_csv(\n",
    "    os.path.join(MIMIC_DIR, \"hosp\", \"microbiologyevents.csv\"),\n",
    "    columns=[\"spec_itemid\", \"test_itemid\", \"test_name\"],\n",
    ")\n",
    "\n",
    "# Create a 'concept_code' column that contains both test_itemid and spec_itemid\n",
    "microbiology_events_df = microbiology_events_df.with_columns(\n",
    "    [pl.concat_list([\"spec_itemid\", \"test_itemid\"]).alias(\"concept_code_list\")]\n",
    ")\n",
    "\n",
    "# Explode the concept_code_list to have one concept_code per row\n",
    "microbiology_events_df = microbiology_events_df.explode(\"concept_code_list\").rename(\n",
    "    {\"concept_code_list\": \"concept_code\"}\n",
    ")\n",
    "\n",
    "# Drop duplicates\n",
    "microbiology_events_df = microbiology_events_df.unique()\n",
    "\n",
    "# Load the microtest_mapping_df\n",
    "microtest_mapping_df = pl.read_csv(\n",
    "    os.path.join(MAPPING_DIR, \"gcpt_micro_microtest.csv\")\n",
    ").join(processed_omop_concepts, on=\"target_concept_id\", how=\"left\")\n",
    "\n",
    "# Join the microbiology_events_df with microtest_mapping_df on the 'concept_code'\n",
    "joined_microtest_df = microbiology_events_df.join(\n",
    "    microtest_mapping_df, on=\"concept_code\", how=\"inner\"\n",
    ")\n",
    "\n",
    "# Group by 'test_name' and count the occurrences\n",
    "test_name_counts = joined_microtest_df.groupby(\"test_name\").agg(\n",
    "    [pl.count().alias(\"count\")]\n",
    ")\n",
    "\n",
    "# Filter to keep only those 'test_name' with exactly one match\n",
    "test_name_filtered = test_name_counts.filter(pl.col(\"count\") == 1).select(\"test_name\")\n",
    "\n",
    "# Join back to get the rows with exactly one match\n",
    "result_df = joined_microtest_df.join(test_name_filtered, on=\"test_name\", how=\"inner\")\n",
    "\n",
    "for row in result_df.iter_rows(named=True):\n",
    "    final_mappings.append(\n",
    "        [\n",
    "            \"gcpt_micro_microtest\",\n",
    "            \"microbiologyevents\",\n",
    "            \"MIMIC_IV_MicrobiologyTest/\",\n",
    "            row[\"test_name\"],\n",
    "            row[\"vocabulary_id\"],\n",
    "            row[\"target_concept_id\"],\n",
    "            row[\"ontology_concept_name\"],\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gcpt_obs_drgcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema correctly\n",
    "drgschema = {\"drg_code\": pl.Utf8}  # Use pl.Utf8 to specify string type\n",
    "\n",
    "# Read the CSV file with the correct schema and select the unique rows\n",
    "drgcodes_df = (\n",
    "    pl.read_csv(\n",
    "        os.path.join(MIMIC_DIR, \"hosp\", \"drgcodes.csv\"),\n",
    "        columns=[\"drg_type\", \"drg_code\", \"description\"],\n",
    "        schema_overrides=drgschema,\n",
    "    )\n",
    "    .unique()\n",
    "    .rename({\"description\": \"concept_code\"})\n",
    ")\n",
    "\n",
    "# Combine \"drg_type\" and \"drg_code\" into a single string column\n",
    "drgcodes_df = drgcodes_df.with_columns(\n",
    "    (pl.col(\"drg_type\") + \"/\" + pl.col(\"drg_code\")).alias(\"source_code\")\n",
    ")\n",
    "\n",
    "# Convert concept_code to lowercase to ensure case-insensitive join\n",
    "drgcodes_df = drgcodes_df.with_columns(\n",
    "    pl.col(\"concept_code\").str.to_lowercase().alias(\"concept_code\")\n",
    ")\n",
    "\n",
    "# Select the combined column\n",
    "drgcodes_df = drgcodes_df.select([\"source_code\", \"concept_code\"])\n",
    "\n",
    "# Load the mapping dataframe and also convert concept_code to lowercase\n",
    "drg_mapping_df = pl.read_csv(\n",
    "    os.path.join(MAPPING_DIR, \"gcpt_obs_drgcodes.csv\")\n",
    ").with_columns(pl.col(\"concept_code\").str.to_lowercase().alias(\"concept_code\"))\n",
    "\n",
    "# Perform the first join between drgcodes_df and drg_mapping_df\n",
    "joined_drg_df = drgcodes_df.join(drg_mapping_df, on=\"concept_code\", how=\"inner\")\n",
    "\n",
    "# Perform the second join with processed_omop_concepts on target_concept_id\n",
    "joined_drg_df = joined_drg_df.join(\n",
    "    processed_omop_concepts, on=\"target_concept_id\", how=\"left\"\n",
    ")\n",
    "\n",
    "# Select the relevant columns and filter out rows where vocabulary_id is null\n",
    "result_df = joined_drg_df.select([*relevant_mapping_cols, \"source_code\"]).filter(\n",
    "    pl.col(\"vocabulary_id\").is_not_null()\n",
    ")\n",
    "\n",
    "for row in result_df.iter_rows(named=True):\n",
    "    final_mappings.append(\n",
    "        [\n",
    "            \"gcpt_obs_drgcodes\",\n",
    "            \"drgcodes\",\n",
    "            \"\",\n",
    "            row[\"source_code\"],\n",
    "            row[\"vocabulary_id\"],\n",
    "            row[\"target_concept_id\"],\n",
    "            row[\"ontology_concept_name\"],\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gcpt_proc_datetimeevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_def_df = pl.read_csv(\n",
    "    os.path.join(MIMIC_DIR, \"icu\", \"d_items.csv\"), columns=[\"itemid\"]\n",
    ").rename({\"itemid\": \"concept_code\"})\n",
    "datetimeevents_mapping_df = pl.read_csv(\n",
    "    os.path.join(MAPPING_DIR, \"gcpt_proc_datetimeevents.csv\")\n",
    ")\n",
    "datetimeevents_mapping_df = datetimeevents_mapping_df.join(\n",
    "    processed_omop_concepts, on=\"target_concept_id\", how=\"left\"\n",
    ")\n",
    "joined_datetimeevents_df = items_def_df.join(\n",
    "    datetimeevents_mapping_df, on=\"concept_code\", how=\"inner\"\n",
    ").select(relevant_mapping_cols)\n",
    "\n",
    "for row in joined_datetimeevents_df.iter_rows(named=True):\n",
    "    final_mappings.append(\n",
    "        [\n",
    "            \"gcpt_proc_datetimeevents\",\n",
    "            \"d_items\",\n",
    "            \"MIMIC_IV_ITEM/\",\n",
    "            row[\"concept_code\"],\n",
    "            row[\"vocabulary_id\"],\n",
    "            row[\"target_concept_id\"],\n",
    "            row[\"ontology_concept_name\"],\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gcpt_proc_itemid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_def_df = pl.read_csv(\n",
    "    os.path.join(MIMIC_DIR, \"icu\", \"d_items.csv\"), columns=[\"itemid\"]\n",
    ").rename({\"itemid\": \"concept_code\"})\n",
    "proc_itemid_mapping_df = pl.read_csv(os.path.join(MAPPING_DIR, \"gcpt_proc_itemid.csv\"))\n",
    "proc_itemid_mapping_df = proc_itemid_mapping_df.join(\n",
    "    processed_omop_concepts, on=\"target_concept_id\", how=\"left\"\n",
    ")\n",
    "joined_proc_itemid_df = items_def_df.join(\n",
    "    proc_itemid_mapping_df, on=\"concept_code\", how=\"inner\"\n",
    ").select(relevant_mapping_cols)\n",
    "\n",
    "for row in joined_proc_itemid_df.iter_rows(named=True):\n",
    "    final_mappings.append(\n",
    "        [\n",
    "            \"gcpt_proc_itemid\",\n",
    "            \"d_items\",\n",
    "            \"MIMIC_IV_ITEM/\",\n",
    "            row[\"concept_code\"],\n",
    "            row[\"vocabulary_id\"],\n",
    "            row[\"target_concept_id\"],\n",
    "            row[\"ontology_concept_name\"],\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gcpt_vis_admission.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admission_mapping_df = pl.read_csv(os.path.join(MAPPING_DIR, \"gcpt_vis_admission.csv\"))\n",
    "# check mapping types\n",
    "print(admission_mapping_df.select(\"source_vocabulary_id\").unique())\n",
    "\n",
    "# Verifiy that admission_type and values of mimic_vis_admission_type match\n",
    "print(\n",
    "    pl.read_csv(os.path.join(MIMIC_DIR, \"hosp\", \"admissions.csv\"))\n",
    "    .select(\"admission_type\")\n",
    "    .unique()\n",
    ")\n",
    "admission_type_df = admission_mapping_df.filter(\n",
    "    pl.col(\"source_vocabulary_id\") == \"mimiciv_vis_admission_type\"\n",
    ")\n",
    "print(admission_type_df[\"concept_code\"])\n",
    "\n",
    "joined_admission_type_df = admission_type_df.join(\n",
    "    processed_omop_concepts, on=\"target_concept_id\", how=\"left\"\n",
    ").select(relevant_mapping_cols)\n",
    "\n",
    "for row in joined_admission_type_df.iter_rows(named=True):\n",
    "    final_mappings.append(\n",
    "        [\n",
    "            \"gcpt_vis_admission\",\n",
    "            \"admissions\",\n",
    "            \"MIMIC_IV_Admission/\",\n",
    "            row[\"concept_code\"],\n",
    "            row[\"vocabulary_id\"],\n",
    "            row[\"target_concept_id\"],\n",
    "            row[\"ontology_concept_name\"],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "discharge_location_df = admission_mapping_df.filter(\n",
    "    pl.col(\"source_vocabulary_id\") == \"mimiciv_vis_discharge_location\"\n",
    ")\n",
    "joined_discharge_location_df = discharge_location_df.join(\n",
    "    processed_omop_concepts, on=\"target_concept_id\", how=\"left\"\n",
    ").select(relevant_mapping_cols)\n",
    "\n",
    "for row in joined_discharge_location_df.iter_rows(named=True):\n",
    "    final_mappings.append(\n",
    "        [\n",
    "            \"gcpt_vis_admission\",\n",
    "            \"admissions\",\n",
    "            \"MIMIC_IV_Discharge_Location/\",\n",
    "            row[\"concept_code\"],\n",
    "            row[\"vocabulary_id\"],\n",
    "            row[\"target_concept_id\"],\n",
    "            row[\"ontology_concept_name\"],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "service_df = admission_mapping_df.filter(\n",
    "    pl.col(\"source_vocabulary_id\") == \"mimiciv_vis_service\"\n",
    ")\n",
    "joined_service_df = service_df.join(\n",
    "    processed_omop_concepts, on=\"target_concept_id\", how=\"left\"\n",
    ").select(relevant_mapping_cols)\n",
    "\n",
    "for row in joined_service_df.iter_rows(named=True):\n",
    "    final_mappings.append(\n",
    "        [\n",
    "            \"gcpt_vis_admission\",\n",
    "            \"services\",\n",
    "            \"MIMIC_IV_Service/\",\n",
    "            row[\"concept_code\"],\n",
    "            row[\"vocabulary_id\"],\n",
    "            row[\"target_concept_id\"],\n",
    "            row[\"ontology_concept_name\"],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "admission_location_df = admission_mapping_df.filter(\n",
    "    pl.col(\"source_vocabulary_id\") == \"mimiciv_vis_admission_location\"\n",
    ")\n",
    "joined_admission_location_df = admission_location_df.join(\n",
    "    processed_omop_concepts, on=\"target_concept_id\", how=\"left\"\n",
    ").select(relevant_mapping_cols)\n",
    "\n",
    "for row in joined_admission_location_df.iter_rows(named=True):\n",
    "    final_mappings.append(\n",
    "        [\n",
    "            \"gcpt_vis_admission\",\n",
    "            \"admissions\",\n",
    "            \"MIMIC_IV_Admission_Location/\",\n",
    "            row[\"concept_code\"],\n",
    "            row[\"vocabulary_id\"],\n",
    "            row[\"target_concept_id\"],\n",
    "            row[\"ontology_concept_name\"],\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also add the ethnicity file as it is clear form where the data came from\n",
    "gcpt_per_ethnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ethnicity_mapping_df = pl.read_csv(os.path.join(MAPPING_DIR, \"gcpt_per_ethnicity.csv\"))\n",
    "\n",
    "joined_ethnicity_mapping_df = ethnicity_mapping_df.join(\n",
    "    processed_omop_concepts, on=\"target_concept_id\", how=\"left\"\n",
    ").select(relevant_mapping_cols)\n",
    "\n",
    "for row in joined_admission_type_df.iter_rows(named=True):\n",
    "    final_mappings.append(\n",
    "        [\n",
    "            \"gcpt_per_ethnicity\",\n",
    "            \"admissions\",\n",
    "            \"MIMIC_IV_Race/\",\n",
    "            row[\"concept_code\"],\n",
    "            row[\"vocabulary_id\"],\n",
    "            row[\"target_concept_id\"],\n",
    "            row[\"ontology_concept_name\"],\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Mappig\n",
    "We also add limited individual mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check available gender column values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.read_csv(os.path.join(MIMIC_DIR, \"hosp\", \"patients.csv\")).select([\"gender\"]).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_mappings = [\n",
    "    [\"individual\", \"patients\", \"MIMIC_IV_Gender/\", \"F\", \"Gender/F\", 8532, \"FEMALE\"],\n",
    "    [\"individual\", \"patients\", \"MIMIC_IV_Gender/\", \"M\", \"Gender/M\", 8507, \"MALE\"],\n",
    "]\n",
    "\n",
    "final_mappings.extend(individual_mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store final mappings in result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the final mappings list into a Polars DataFrame\n",
    "final_mappings_df = pl.DataFrame(\n",
    "    final_mappings,\n",
    "    schema=[\n",
    "        \"mapping_source_table\",\n",
    "        \"table_name\",\n",
    "        \"etl_prefix\",\n",
    "        \"source_code\",\n",
    "        \"vocabulary_id\",\n",
    "        \"target_concept_id\",\n",
    "        \"ontology_concept_name\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "final_mappings_df.write_csv(\n",
    "    os.path.join(MAPPING_DIR, \"mapping_results\", \"final_mappings.csv\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
